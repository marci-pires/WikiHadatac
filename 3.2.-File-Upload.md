We call _data uploading_  the process of adding content into the data repository, and _data ingestion_ the process of collecting, organizing, indexing content's metadata, and as important, the process of connecting the newly organized metadata to the content. 

Using traditional databases developers are requires to develop one very large schema to manage both the data content and its associated metadata, and to make sure that content identifiers and corresponding metadata are properly connected.	 
The use of ontologies to guide the process of organizing metadata alleviates the task of organizing the metadata since the data becomes self-organizing when values are tagged with terms from a common set of ontologies. Furthermore, ontologies also help the process of logically connecting data to newly created graphs inside of the metadata repository.

![](https://raw.githubusercontent.com/paulopinheiro1234/hadatac-screenshots/master/Sec3/DatafileManagement.png)

The screenshot above shows how data files can be uploaded into HADatAc in ways very similar to other well established cloud-based online repositories like Dropbox and Google drive. For instance, the “DA-CBIS-Feb-2017-DMG-B-3.csv” file, which is a data file, was uploaded into the infrastructure and now it is assigned to a user who can leave the datafile there or may decide to share, download or even delete the file. 

Once files are uploaded, they can either be automatically ingested in case the infrastructure’s knowledge base knows how to process the file, or may guide scientists in the process of telling the system how to ingest the data. 
